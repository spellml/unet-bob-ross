{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /spell/scripts/upgrade_env.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /spell/scripts/upgrade_env.sh\n",
    "pip install -U --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html\n",
    "pip install -U tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run instructions:\n",
    "\n",
    "```bash\n",
    "prodspell run \\\n",
    "  --machine-type t4 \\\n",
    "  --github-url https://github.com/ResidentMario/spell-unet-bob-ross.git \\\n",
    "  --pip kaggle \\\n",
    "  --tensorboard-dir /spell/tensorboards/model_3 \\\n",
    "  \"chmod +x /spell/scripts/download_data.sh; chmod +x /spell/scripts/upgrade_env.sh; /spell/scripts/download_data.sh; /spell/scripts/upgrade_env.sh; python /spell/models/model_3.py\"\n",
    "```\n",
    "\n",
    "```bash\n",
    "prodspell run \\\n",
    "  --machine-type t4 \\\n",
    "  --github-url https://github.com/ResidentMario/spell-unet-bob-ross.git \\\n",
    "  --pip kaggle \\\n",
    "  --tensorboard-dir /spell/tensorboards/model_4 \\\n",
    "  \"chmod +x /spell/scripts/download_data.sh; chmod +x /spell/scripts/upgrade_env.sh; /spell/scripts/download_data.sh; /spell/scripts/upgrade_env.sh; python /spell/models/model_4.py\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## writeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /spell/models/model_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /spell/models/model_3.py\n",
    "# number of epochs of training\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "class BobRossSegmentedImagesDataset(Dataset):\n",
    "    def __init__(self, dataroot):\n",
    "        super().__init__()\n",
    "        self.dataroot = dataroot\n",
    "        self.imgs = list((self.dataroot / 'train' / 'images').rglob('*.png'))\n",
    "        self.segs = list((self.dataroot / 'train' / 'labels').rglob('*.png'))\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((164, 164)),\n",
    "            transforms.Pad(46, padding_mode='reflect'),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                            mean=(0.459387, 0.46603974, 0.4336706),\n",
    "                            std=(0.06098535, 0.05802868, 0.08737113)\n",
    "            )\n",
    "        ])\n",
    "        self.color_key = {\n",
    "            3 : 0,\n",
    "            5: 1,\n",
    "            10: 2,\n",
    "            14: 3,\n",
    "            17: 4,\n",
    "            18: 5,\n",
    "            22: 6,\n",
    "            27: 7,\n",
    "            61: 8\n",
    "        }        \n",
    "        assert len(self.imgs) == len(self.segs)\n",
    "        # TODO: remean images to N(0, 1)?\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        def translate(x):\n",
    "            return self.color_key[x]\n",
    "        translate = np.vectorize(translate)\n",
    "        \n",
    "        img = Image.open(self.imgs[i])\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        seg = Image.open(self.segs[i])\n",
    "        seg = seg.resize((256, 256), Image.NEAREST)\n",
    "        \n",
    "        # Labels are in the ADE20K ontology and are not consequetive,\n",
    "        # we have to apply a remap operation over the labels in a just-in-time\n",
    "        # manner. This slows things down, but it's fine, this is just a demo\n",
    "        # anyway.\n",
    "        seg = translate(np.array(seg)).astype('int64')\n",
    "        \n",
    "        # One-hot encode the segmentation mask.\n",
    "        # def ohe_mat(segmap):\n",
    "        #     return np.array(\n",
    "        #         list(\n",
    "        #             np.array(segmap) == i for i in range(9)\n",
    "        #         )\n",
    "        #     ).astype(int).reshape(9, 256, 256)\n",
    "        # seg = ohe_mat(seg)\n",
    "        \n",
    "        # Additionally, the original UNet implementation outputs a segmentation map\n",
    "        # for a subset of the overall image, not the image as a whole! With this input\n",
    "        # size the segmentation map targeted is a (164, 164) center crop.\n",
    "        seg = seg[46:210, 46:210]\n",
    "        \n",
    "        return img, seg\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_1_1 = nn.Conv2d(3, 64, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_1_1.weight)\n",
    "        self.relu_1_2 = nn.ReLU()\n",
    "        self.norm_1_3 = nn.BatchNorm2d(64)\n",
    "        self.conv_1_4 = nn.Conv2d(64, 64, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_1_4.weight)\n",
    "        self.relu_1_5 = nn.ReLU()\n",
    "        self.norm_1_6 = nn.BatchNorm2d(64)\n",
    "        self.pool_1_7 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv_2_1 = nn.Conv2d(64, 128, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_2_1.weight)        \n",
    "        self.relu_2_2 = nn.ReLU()\n",
    "        self.norm_2_3 = nn.BatchNorm2d(128)\n",
    "        self.conv_2_4 = nn.Conv2d(128, 128, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_2_4.weight)        \n",
    "        self.relu_2_5 = nn.ReLU()\n",
    "        self.norm_2_6 = nn.BatchNorm2d(128)\n",
    "        self.pool_2_7 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv_3_1 = nn.Conv2d(128, 256, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_3_1.weight)\n",
    "        self.relu_3_2 = nn.ReLU()\n",
    "        self.norm_3_3 = nn.BatchNorm2d(256)\n",
    "        self.conv_3_4 = nn.Conv2d(256, 256, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_3_4.weight)\n",
    "        self.relu_3_5 = nn.ReLU()\n",
    "        self.norm_3_6 = nn.BatchNorm2d(256)\n",
    "        self.pool_3_7 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv_4_1 = nn.Conv2d(256, 512, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_4_1.weight)\n",
    "        self.relu_4_2 = nn.ReLU()\n",
    "        self.norm_4_3 = nn.BatchNorm2d(512)\n",
    "        self.conv_4_4 = nn.Conv2d(512, 512, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_4_4.weight)\n",
    "        self.relu_4_5 = nn.ReLU()\n",
    "        self.norm_4_6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # deconv is the '2D transposed convolution operator'\n",
    "        self.deconv_5_1 = nn.ConvTranspose2d(512, 256, (2, 2), 2)\n",
    "        # 61x61 -> 48x48 crop\n",
    "        self.c_crop_5_2 = lambda x: x[:, :, 6:54, 6:54]\n",
    "        self.concat_5_3 = lambda x, y: torch.cat((x, y), dim=1)\n",
    "        self.conv_5_4 = nn.Conv2d(512, 256, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_5_4.weight)        \n",
    "        self.relu_5_5 = nn.ReLU()\n",
    "        self.norm_5_6 = nn.BatchNorm2d(256)\n",
    "        self.conv_5_7 = nn.Conv2d(256, 256, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_5_7.weight)\n",
    "        self.relu_5_8 = nn.ReLU()\n",
    "        self.norm_5_9 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.deconv_6_1 = nn.ConvTranspose2d(256, 128, (2, 2), 2)\n",
    "        # 121x121 -> 88x88 crop\n",
    "        self.c_crop_6_2 = lambda x: x[:, :, 17:105, 17:105]\n",
    "        self.concat_6_3 = lambda x, y: torch.cat((x, y), dim=1)\n",
    "        self.conv_6_4 = nn.Conv2d(256, 128, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_6_4.weight)\n",
    "        self.relu_6_5 = nn.ReLU()\n",
    "        self.norm_6_6 = nn.BatchNorm2d(128)\n",
    "        self.conv_6_7 = nn.Conv2d(128, 128, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_6_7.weight)\n",
    "        self.relu_6_8 = nn.ReLU()\n",
    "        self.norm_6_9 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.deconv_7_1 = nn.ConvTranspose2d(128, 64, (2, 2), 2)\n",
    "        # 252x252 -> 168x168 crop\n",
    "        self.c_crop_7_2 = lambda x: x[:, :, 44:212, 44:212]\n",
    "        self.concat_7_3 = lambda x, y: torch.cat((x, y), dim=1)\n",
    "        self.conv_7_4 = nn.Conv2d(128, 64, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_7_4.weight)\n",
    "        self.relu_7_5 = nn.ReLU()\n",
    "        self.norm_7_6 = nn.BatchNorm2d(64)\n",
    "        self.conv_7_7 = nn.Conv2d(64, 64, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_7_7.weight)        \n",
    "        self.relu_7_8 = nn.ReLU()\n",
    "        self.norm_7_9 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # 1x1 conv ~= fc; n_classes = 9\n",
    "        self.conv_8_1 = nn.Conv2d(64, 9, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1_1(x)\n",
    "        x = self.relu_1_2(x)\n",
    "        x = self.norm_1_3(x)\n",
    "        x = self.conv_1_4(x)\n",
    "        x = self.relu_1_5(x)\n",
    "        x_residual_1 = self.norm_1_6(x)\n",
    "        x = self.pool_1_7(x_residual_1)\n",
    "        \n",
    "        x = self.conv_2_1(x)\n",
    "        x = self.relu_2_2(x)\n",
    "        x = self.norm_2_3(x)\n",
    "        x = self.conv_2_4(x)\n",
    "        x = self.relu_2_5(x)\n",
    "        x_residual_2 = self.norm_2_6(x)\n",
    "        x = self.pool_2_7(x_residual_2)\n",
    "        \n",
    "        x = self.conv_3_1(x)\n",
    "        x = self.relu_3_2(x)\n",
    "        x = self.norm_3_3(x)\n",
    "        x = self.conv_3_4(x)\n",
    "        x = self.relu_3_5(x)\n",
    "        x_residual_3 = self.norm_3_6(x)\n",
    "        x = self.pool_3_7(x_residual_3)\n",
    "        \n",
    "        x = self.conv_4_1(x)\n",
    "        x = self.relu_4_2(x)\n",
    "        x = self.norm_4_3(x)        \n",
    "        x = self.conv_4_4(x)\n",
    "        x = self.relu_4_5(x)\n",
    "        x = self.norm_4_6(x)\n",
    "        \n",
    "        x = self.deconv_5_1(x)\n",
    "        x = self.concat_5_3(self.c_crop_5_2(x_residual_3), x)\n",
    "        x = self.conv_5_4(x)\n",
    "        x = self.relu_5_5(x)\n",
    "        x = self.norm_5_6(x)\n",
    "        x = self.conv_5_7(x)\n",
    "        x = self.relu_5_8(x)\n",
    "        x = self.norm_5_9(x)\n",
    "        \n",
    "        x = self.deconv_6_1(x)\n",
    "        x = self.concat_6_3(self.c_crop_6_2(x_residual_2), x)\n",
    "        x = self.conv_6_4(x)\n",
    "        x = self.relu_6_5(x)\n",
    "        x = self.norm_6_6(x)\n",
    "        x = self.conv_6_7(x)\n",
    "        x = self.relu_6_8(x)\n",
    "        x = self.norm_6_9(x)\n",
    "        \n",
    "        x = self.deconv_7_1(x)\n",
    "        x = self.concat_7_3(self.c_crop_7_2(x_residual_1), x)\n",
    "        x = self.conv_7_4(x)\n",
    "        x = self.relu_7_5(x)\n",
    "        x = self.norm_7_6(x)\n",
    "        x = self.conv_7_7(x)\n",
    "        x = self.relu_7_8(x)\n",
    "        x = self.norm_7_9(x)\n",
    "        \n",
    "        x = self.conv_8_1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "dataroot = Path('/mnt/segmented-bob-ross-images/')\n",
    "dataset = BobRossSegmentedImagesDataset(dataroot)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=8)\n",
    "\n",
    "writer = SummaryWriter(f'/spell/tensorboards/model_3')\n",
    "\n",
    "model = UNet()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    losses = []\n",
    "\n",
    "    for i, (batch, segmap) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch = batch.cuda()\n",
    "        segmap = segmap.cuda()\n",
    "\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, segmap)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        curr_loss = loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print(\n",
    "                f'Finished epoch {epoch}, batch {i}. Loss: {curr_loss:.3f}.'\n",
    "            )\n",
    "\n",
    "        writer.add_scalar(\n",
    "            'training loss', curr_loss, epoch * len(dataloader) + i\n",
    "        )\n",
    "        losses.append(curr_loss)\n",
    "\n",
    "    print(\n",
    "        f'Finished epoch {epoch}. '\n",
    "        f'avg loss: {np.mean(losses)}; median loss: {np.min(losses)}'\n",
    "    )\n",
    "    if not os.path.exists('/spell/checkpoints/'):\n",
    "        os.mkdir('/spell/checkpoints/')\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model.state_dict(), f'/spell/checkpoints/model_{epoch}.pth')\n",
    "\n",
    "torch.save(model.state_dict(), f'/spell/checkpoints/model_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /spell/models/model_4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /spell/models/model_4.py\n",
    "# number of epochs of training\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "class BobRossSegmentedImagesDataset(Dataset):\n",
    "    def __init__(self, dataroot):\n",
    "        super().__init__()\n",
    "        self.dataroot = dataroot\n",
    "        self.imgs = list((self.dataroot / 'train' / 'images').rglob('*.png'))\n",
    "        self.segs = list((self.dataroot / 'train' / 'labels').rglob('*.png'))\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((164, 164)),\n",
    "            transforms.Pad(46, padding_mode='reflect'),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                            mean=(0.459387, 0.46603974, 0.4336706),\n",
    "                            std=(0.06098535, 0.05802868, 0.08737113)\n",
    "            )\n",
    "        ])\n",
    "        self.color_key = {\n",
    "            3 : 0,\n",
    "            5: 1,\n",
    "            10: 2,\n",
    "            14: 3,\n",
    "            17: 4,\n",
    "            18: 5,\n",
    "            22: 6,\n",
    "            27: 7,\n",
    "            61: 8\n",
    "        }        \n",
    "        assert len(self.imgs) == len(self.segs)\n",
    "        # TODO: remean images to N(0, 1)?\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        def translate(x):\n",
    "            return self.color_key[x]\n",
    "        translate = np.vectorize(translate)\n",
    "        \n",
    "        img = Image.open(self.imgs[i])\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        seg = Image.open(self.segs[i])\n",
    "        seg = seg.resize((256, 256), Image.NEAREST)\n",
    "        \n",
    "        # Labels are in the ADE20K ontology and are not consequetive,\n",
    "        # we have to apply a remap operation over the labels in a just-in-time\n",
    "        # manner. This slows things down, but it's fine, this is just a demo\n",
    "        # anyway.\n",
    "        seg = translate(np.array(seg)).astype('int64')\n",
    "        \n",
    "        # One-hot encode the segmentation mask.\n",
    "        # def ohe_mat(segmap):\n",
    "        #     return np.array(\n",
    "        #         list(\n",
    "        #             np.array(segmap) == i for i in range(9)\n",
    "        #         )\n",
    "        #     ).astype(int).reshape(9, 256, 256)\n",
    "        # seg = ohe_mat(seg)\n",
    "        \n",
    "        # Additionally, the original UNet implementation outputs a segmentation map\n",
    "        # for a subset of the overall image, not the image as a whole! With this input\n",
    "        # size the segmentation map targeted is a (164, 164) center crop.\n",
    "        seg = seg[46:210, 46:210]\n",
    "        \n",
    "        return img, seg\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_1_1 = nn.Conv2d(3, 64, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_1_1.weight)\n",
    "        self.relu_1_2 = nn.ReLU()\n",
    "        self.norm_1_3 = nn.BatchNorm2d(64)\n",
    "        self.conv_1_4 = nn.Conv2d(64, 64, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_1_4.weight)\n",
    "        self.relu_1_5 = nn.ReLU()\n",
    "        self.norm_1_6 = nn.BatchNorm2d(64)\n",
    "        self.pool_1_7 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv_2_1 = nn.Conv2d(64, 128, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_2_1.weight)        \n",
    "        self.relu_2_2 = nn.ReLU()\n",
    "        self.norm_2_3 = nn.BatchNorm2d(128)\n",
    "        self.conv_2_4 = nn.Conv2d(128, 128, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_2_4.weight)        \n",
    "        self.relu_2_5 = nn.ReLU()\n",
    "        self.norm_2_6 = nn.BatchNorm2d(128)\n",
    "        self.pool_2_7 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv_3_1 = nn.Conv2d(128, 256, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_3_1.weight)\n",
    "        self.relu_3_2 = nn.ReLU()\n",
    "        self.norm_3_3 = nn.BatchNorm2d(256)\n",
    "        self.conv_3_4 = nn.Conv2d(256, 256, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_3_4.weight)\n",
    "        self.relu_3_5 = nn.ReLU()\n",
    "        self.norm_3_6 = nn.BatchNorm2d(256)\n",
    "        self.pool_3_7 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv_4_1 = nn.Conv2d(256, 512, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_4_1.weight)\n",
    "        self.relu_4_2 = nn.ReLU()\n",
    "        self.norm_4_3 = nn.BatchNorm2d(512)\n",
    "        self.conv_4_4 = nn.Conv2d(512, 512, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_4_4.weight)\n",
    "        self.relu_4_5 = nn.ReLU()\n",
    "        self.norm_4_6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # deconv is the '2D transposed convolution operator'\n",
    "        self.deconv_5_1 = nn.ConvTranspose2d(512, 256, (2, 2), 2)\n",
    "        # 61x61 -> 48x48 crop\n",
    "        self.c_crop_5_2 = lambda x: x[:, :, 6:54, 6:54]\n",
    "        self.concat_5_3 = lambda x, y: torch.cat((x, y), dim=1)\n",
    "        self.conv_5_4 = nn.Conv2d(512, 256, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_5_4.weight)        \n",
    "        self.relu_5_5 = nn.ReLU()\n",
    "        self.norm_5_6 = nn.BatchNorm2d(256)\n",
    "        self.conv_5_7 = nn.Conv2d(256, 256, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_5_7.weight)\n",
    "        self.relu_5_8 = nn.ReLU()\n",
    "        self.norm_5_9 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.deconv_6_1 = nn.ConvTranspose2d(256, 128, (2, 2), 2)\n",
    "        # 121x121 -> 88x88 crop\n",
    "        self.c_crop_6_2 = lambda x: x[:, :, 17:105, 17:105]\n",
    "        self.concat_6_3 = lambda x, y: torch.cat((x, y), dim=1)\n",
    "        self.conv_6_4 = nn.Conv2d(256, 128, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_6_4.weight)\n",
    "        self.relu_6_5 = nn.ReLU()\n",
    "        self.norm_6_6 = nn.BatchNorm2d(128)\n",
    "        self.conv_6_7 = nn.Conv2d(128, 128, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_6_7.weight)\n",
    "        self.relu_6_8 = nn.ReLU()\n",
    "        self.norm_6_9 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.deconv_7_1 = nn.ConvTranspose2d(128, 64, (2, 2), 2)\n",
    "        # 252x252 -> 168x168 crop\n",
    "        self.c_crop_7_2 = lambda x: x[:, :, 44:212, 44:212]\n",
    "        self.concat_7_3 = lambda x, y: torch.cat((x, y), dim=1)\n",
    "        self.conv_7_4 = nn.Conv2d(128, 64, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_7_4.weight)\n",
    "        self.relu_7_5 = nn.ReLU()\n",
    "        self.norm_7_6 = nn.BatchNorm2d(64)\n",
    "        self.conv_7_7 = nn.Conv2d(64, 64, 3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_7_7.weight)        \n",
    "        self.relu_7_8 = nn.ReLU()\n",
    "        self.norm_7_9 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # 1x1 conv ~= fc; n_classes = 9\n",
    "        self.conv_8_1 = nn.Conv2d(64, 9, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1_1(x)\n",
    "        x = self.relu_1_2(x)\n",
    "        x = self.norm_1_3(x)\n",
    "        x = self.conv_1_4(x)\n",
    "        x = self.relu_1_5(x)\n",
    "        x_residual_1 = self.norm_1_6(x)\n",
    "        x = self.pool_1_7(x_residual_1)\n",
    "        \n",
    "        x = self.conv_2_1(x)\n",
    "        x = self.relu_2_2(x)\n",
    "        x = self.norm_2_3(x)\n",
    "        x = self.conv_2_4(x)\n",
    "        x = self.relu_2_5(x)\n",
    "        x_residual_2 = self.norm_2_6(x)\n",
    "        x = self.pool_2_7(x_residual_2)\n",
    "        \n",
    "        x = self.conv_3_1(x)\n",
    "        x = self.relu_3_2(x)\n",
    "        x = self.norm_3_3(x)\n",
    "        x = self.conv_3_4(x)\n",
    "        x = self.relu_3_5(x)\n",
    "        x_residual_3 = self.norm_3_6(x)\n",
    "        x = self.pool_3_7(x_residual_3)\n",
    "        \n",
    "        x = self.conv_4_1(x)\n",
    "        x = self.relu_4_2(x)\n",
    "        x = self.norm_4_3(x)        \n",
    "        x = self.conv_4_4(x)\n",
    "        x = self.relu_4_5(x)\n",
    "        x = self.norm_4_6(x)\n",
    "        \n",
    "        x = self.deconv_5_1(x)\n",
    "        x = self.concat_5_3(self.c_crop_5_2(x_residual_3), x)\n",
    "        x = self.conv_5_4(x)\n",
    "        x = self.relu_5_5(x)\n",
    "        x = self.norm_5_6(x)\n",
    "        x = self.conv_5_7(x)\n",
    "        x = self.relu_5_8(x)\n",
    "        x = self.norm_5_9(x)\n",
    "        \n",
    "        x = self.deconv_6_1(x)\n",
    "        x = self.concat_6_3(self.c_crop_6_2(x_residual_2), x)\n",
    "        x = self.conv_6_4(x)\n",
    "        x = self.relu_6_5(x)\n",
    "        x = self.norm_6_6(x)\n",
    "        x = self.conv_6_7(x)\n",
    "        x = self.relu_6_8(x)\n",
    "        x = self.norm_6_9(x)\n",
    "        \n",
    "        x = self.deconv_7_1(x)\n",
    "        x = self.concat_7_3(self.c_crop_7_2(x_residual_1), x)\n",
    "        x = self.conv_7_4(x)\n",
    "        x = self.relu_7_5(x)\n",
    "        x = self.norm_7_6(x)\n",
    "        x = self.conv_7_7(x)\n",
    "        x = self.relu_7_8(x)\n",
    "        x = self.norm_7_9(x)\n",
    "        \n",
    "        x = self.conv_8_1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "dataroot = Path('/mnt/segmented-bob-ross-images/')\n",
    "dataset = BobRossSegmentedImagesDataset(dataroot)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=8)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "writer = SummaryWriter(f'/spell/tensorboards/model_4')\n",
    "\n",
    "model = UNet()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    losses = []\n",
    "\n",
    "    for i, (batch, segmap) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch = batch.cuda()\n",
    "        segmap = segmap.cuda()\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, segmap)\n",
    "        # output = model(batch)\n",
    "        # loss = criterion(output, segmap)\n",
    "\n",
    "        # scaler is a GradScaler object. This is the other component of the PyTorch mixed\n",
    "        # precision training API. We scale the losses _before_ apply backprop, this helps\n",
    "        # to ensure the model does not diverge due to very small weight updates rounding\n",
    "        # down to zero.\n",
    "        #\n",
    "        # scaler uses an interesting backoff algorithm to determine when it can scale the\n",
    "        # gradient update multiplier up versus when it needs to cut back on that activity.\n",
    "        # when it sees the model update diverge the other way (e.g. a nan or inf value) it\n",
    "        # reduces its multiplier value and discards the offending gradient update.\n",
    "        scaler.scale(loss).backward()\n",
    "        # loss.backward()\n",
    "\n",
    "        # the scalar wraps the optimizer in order to implements its discarding behavior\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # optimizer.step()\n",
    "        \n",
    "        curr_loss = loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print(\n",
    "                f'Finished epoch {epoch}, batch {i}. Loss: {curr_loss:.3f}.'\n",
    "            )\n",
    "\n",
    "        writer.add_scalar(\n",
    "            'training loss', curr_loss, epoch * len(dataloader) + i\n",
    "        )\n",
    "        losses.append(curr_loss)\n",
    "\n",
    "    print(\n",
    "        f'Finished epoch {epoch}. '\n",
    "        f'avg loss: {np.mean(losses)}; median loss: {np.min(losses)}'\n",
    "    )\n",
    "    if not os.path.exists('/spell/checkpoints/'):\n",
    "        os.mkdir('/spell/checkpoints/')\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model.state_dict(), f'/spell/checkpoints/model_{epoch}.pth')\n",
    "\n",
    "torch.save(model.state_dict(), f'/spell/checkpoints/model_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 7703497.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
